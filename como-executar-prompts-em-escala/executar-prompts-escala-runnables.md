# Como Executar Prompts em Escala com LangChain Runnables e OpenAI

Refira para o folder CBO-RUNNABLES para um codigo em produto
e no folder como-executar-prompts-em-escala/pratique para um codigo simples de pratica

Este guia apresenta como utilizar **LangChain Runnables** com a **OpenAI** para executar prompts em escala de forma eficiente, com base no projeto de classifica√ß√£o de c√≥digos CBO.

* Para um exemplo de aplica√ß√£o em produ√ß√£o, consulte o diret√≥rio `CBO-RUNNABLES`.
* Para um exemplo simples e did√°tico, acesse o diret√≥rio `como-executar-prompts-em-escala/pratique`.

## Por que usar Runnables para Processamento em Escala?

Quando voc√™ tem **dezenas de milhares de prompts** para processar ou precisa criar **milhares de prompts customizados**, o processamento sequencial se torna invi√°vel. Imagine cen√°rios como:

- **Classificar 50.000 CBOs** com an√°lises personalizadas
- **Processar milhares de documentos** para extra√ß√£o de dados
- **Gerar conte√∫do personalizado** para dezenas de milhares de usu√°rios
- **Analisar grandes datasets** com prompts espec√≠ficos para cada registro

**Sem Runnables**: Processar 10.000 prompts sequencialmente levaria ~10 horas (assumindo 3.6s por prompt)
**Com Runnables**: O mesmo processamento pode ser reduzido para ~1-2 horas usando paraleliza√ß√£o

### Benef√≠cios dos Runnables:

- ‚ö° **Performance**: Processamento paralelo autom√°tico
- üí∞ **Economia**: Melhor controle de custos e tokens
- üîÑ **Robustez**: Tratamento de erros e retry autom√°tico
- üìä **Monitoramento**: Tracking detalhado de progresso
- üîß **Flexibilidade**: Componentes reutiliz√°veis e compos√°veis

## Vis√£o Geral dos Passos

Este guia est√° organizado em etapas progressivas para voc√™ dominar o processamento em escala:

### üéØ **Fundamentos (Passos 1-3)**

- **Configura√ß√£o**: Setup do ambiente e depend√™ncias
- **B√°sicos**: Cria√ß√£o de chains simples com Runnables
- **Composi√ß√£o**: Combina√ß√£o de componentes usando operador pipe (`|`)

### ‚ö° **Paraleliza√ß√£o (Passos 4-5)**

- **RunnableMap**: Processamento paralelo de m√∫ltiplos inputs
- **RunnableParallel**: Execu√ß√£o simult√¢nea de diferentes an√°lises

### üí° **Otimiza√ß√£o (Passos 6-8)**

- **Controle de Tokens**: Monitoramento e c√°lculo de custos
- **Streaming**: Processamento ass√≠ncrono e em tempo real
- **Configura√ß√£o Avan√ßada**: Callbacks, tags e metadados

### üèóÔ∏è **Produ√ß√£o (Passos 9-11)**

- **Exemplo Completo**: Classe para processamento industrial
- **Melhores Pr√°ticas**: Rate limiting, cache e tratamento de erros
- **Monitoramento**: Debugging e logging em produ√ß√£o

## O que s√£o Runnables?

**Runnables** s√£o a interface padr√£o do LangChain para criar componentes compos√°veis e interconect√°veis. Eles implementam m√©todos como `invoke`, `batch`, `stream` e fornecem a base para construir pipelines de processamento de linguagem natural.

### Principais caracter√≠sticas dos Runnables:

- **Composi√ß√£o**: Podem ser combinados usando o operador `|` (pipe)
- **Configura√ß√£o**: Suportam configura√ß√£o via `RunnableConfig`
- **Paraleliza√ß√£o**: Executam opera√ß√µes de forma eficiente
- **Streaming**: Suportam sa√≠da em tempo real
- **Callbacks**: Propagam callbacks automaticamente

## Pr√©-requisitos

```bash
pip install langchain-core langchain-openai openai tqdm python-dotenv tiktoken
```

## Configura√ß√£o do Ambiente

### 1. Arquivo .env

Crie um arquivo `.env` na raiz do projeto:

```env
# Chave de API OpenAI (obrigat√≥ria)
OPENAI_API_KEY=sk-your_openai_api_key_here

# Modelo a ser usado (opcional, padr√£o: gpt-4o)
MODEL=gpt-4o

# Configura√ß√µes de temperatura (opcional)
TEMPERATURE=0
```

### 2. Importa√ß√µes B√°sicas

```python
import os
from dotenv import load_dotenv
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableMap, RunnableLambda, RunnableParallel
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
import tiktoken

# Carregar vari√°veis de ambiente
load_dotenv()
```

## Estrutura B√°sica de um Runnable com OpenAI

### 1. Inicializando o Modelo OpenAI

```python
# Obter configura√ß√µes do ambiente
api_key = os.getenv("OPENAI_API_KEY")
model_name = os.getenv("MODEL", "gpt-4o")

if not api_key:
    raise ValueError("OPENAI_API_KEY n√£o encontrada no arquivo .env")

# Inicializar o modelo OpenAI como Runnable
model = ChatOpenAI(
    model=model_name, 
    temperature=0, 
    api_key=api_key
)

print(f"Usando modelo: {model_name}")
```

**üí° Sum√°rio:** Configuramos o modelo ChatOpenAI como um Runnable, obtendo as credenciais do arquivo .env e definindo par√¢metros como temperatura. Este modelo ser√° a base de todas as chains que criaremos.

### 2. Criando Templates de Prompt

```python
# Template b√°sico para an√°lise de CBO
def create_cbo_prompt_template():
    template = """
    Analise o CBO {cbo_code} ({description}) e responda em formato JSON:
  
    {{
      "an√°lise": "An√°lise detalhada da ocupa√ß√£o",
      "trabalho_extenuante_estressante": "sim/n√£o/moderado",
      "baixa_escolaridade_exigida": "sim/n√£o/moderado",
      "sazonalidade": "sim/n√£o/moderado",
      "alta_exigencia_tecnica": "sim/n√£o/moderado",
      "salario_mais_elevado": "sim/n√£o/moderado",
      "interesse_em_reter_talentos": "sim/n√£o/moderado",
      "alta_rotatividade": "sim/n√£o/moderado"
    }}
    """
  
    return ChatPromptTemplate.from_messages([
        ("system", "Voc√™ √© um especialista em an√°lise de mercado de trabalho brasileiro."),
        ("human", template)
    ])
```

**üí° Sum√°rio:** Criamos um template reutiliz√°vel que define a estrutura do prompt e o formato de resposta esperado. O template usa vari√°veis (`{cbo_code}`, `{description}`) que ser√£o preenchidas dinamicamente para cada CBO.

### 3. Compondo uma Chain B√°sica

```python
# Criar template de prompt
prompt_template = create_cbo_prompt_template()

# Criar parser de sa√≠da
output_parser = StrOutputParser()

# Compor a chain usando o operador pipe (|)
chain = prompt_template | model | output_parser

# Invocar a chain
result = chain.invoke({
    "cbo_code": "2142-05", 
    "description": "Engenheiro de software"
})

print(result)
```

**üí° Sum√°rio:** Combinamos tr√™s componentes (prompt template + modelo + parser) usando o operador pipe (`|`) para criar uma chain completa. Esta chain pode ser invocada com dados e retornar√° a resposta processada e formatada.

## Processamento em Paralelo com RunnableMap

### 1. Processamento M√∫ltiplo

```python
# Fun√ß√£o para criar entrada individual
def create_input(cbo_code, description):
    return {
        "cbo_code": cbo_code,
        "description": description
    }

# Lista de CBOs para processar
cbos_data = [
    ("2142-05", "Engenheiro de software"),
    ("3171-05", "T√©cnico em inform√°tica"),
    ("5111-05", "Vendedor de com√©rcio varejista")
]

# Preparar inputs
inputs = [create_input(code, desc) for code, desc in cbos_data]

# Usar map() para processamento paralelo
map_chain = chain.map()

# Processar todos os inputs em paralelo
results = map_chain.invoke(inputs)

for i, result in enumerate(results):
    print(f"CBO {cbos_data[i][0]}: {result[:100]}...")
```

**üí° Sum√°rio:** Usando `chain.map()`, transformamos nossa chain simples em um processador paralelo que pode lidar com m√∫ltiplos inputs simultaneamente, drasticamente reduzindo o tempo total de processamento.

### 2. Processamento com RunnableParallel

```python
from langchain_core.runnables import RunnableParallel

# Criar chains paralelas para diferentes an√°lises
analysis_chain = RunnableParallel({
    "basic_analysis": prompt_template | model | output_parser,
    "token_count": RunnableLambda(lambda x: count_tokens(str(x))),
    "metadata": RunnableLambda(lambda x: {
        "cbo": x["cbo_code"],
        "timestamp": "2024-01-01"
    })
})

# Executar an√°lises paralelas
parallel_result = analysis_chain.invoke({
    "cbo_code": "2142-05", 
    "description": "Engenheiro de software"
})

print("An√°lise:", parallel_result["basic_analysis"])
print("Tokens:", parallel_result["token_count"])
print("Metadata:", parallel_result["metadata"])
```

**üí° Sum√°rio:** RunnableParallel permite executar diferentes an√°lises simultaneamente no mesmo input, retornando um dicion√°rio com todos os resultados. Ideal para quando voc√™ precisa de m√∫ltiplas perspectivas dos mesmos dados.

## Controle de Tokens e Custos

### 1. Fun√ß√£o de Contagem de Tokens

```python
def count_tokens(text, model="gpt-4o"):
    """Conta tokens usando tiktoken"""
    try:
        encoder = tiktoken.encoding_for_model(model)
        return len(encoder.encode(text))
    except:
        encoder = tiktoken.get_encoding("cl100k_base")
        return len(encoder.encode(text))

# Criar Runnable para controle de tokens
def create_token_aware_chain():
    def process_with_tokens(inputs):
        # Contar tokens de entrada
        input_text = str(inputs)
        input_tokens = count_tokens(input_text)
    
        # Processar com a chain
        result = chain.invoke(inputs)
    
        # Contar tokens de sa√≠da
        output_tokens = count_tokens(result)
    
        return {
            "result": result,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "total_tokens": input_tokens + output_tokens
        }
  
    return RunnableLambda(process_with_tokens)

# Usar chain com controle de tokens
token_chain = create_token_aware_chain()
token_result = token_chain.invoke({
    "cbo_code": "2142-05", 
    "description": "Engenheiro de software"
})

print(f"Tokens de entrada: {token_result['input_tokens']}")
print(f"Tokens de sa√≠da: {token_result['output_tokens']}")
print(f"Total: {token_result['total_tokens']}")
```

**üí° Sum√°rio:** Criamos uma fun√ß√£o wrapper que conta tokens de entrada e sa√≠da usando tiktoken, permitindo monitoramento preciso do uso de tokens para controle de custos e performance.

### 2. C√°lculo de Custos

```python
def calculate_costs(input_tokens, output_tokens, model_name="gpt-4o"):
    """Calcula custos baseado no modelo"""
    pricing = {
        "gpt-4o": {"input": 2.5, "output": 10.0},
        "gpt-3.5-turbo": {"input": 0.5, "output": 1.5},
        "gpt-4": {"input": 30.0, "output": 60.0}
    }
  
    rates = pricing.get(model_name, {"input": 2.5, "output": 10.0})
  
    input_cost = (input_tokens / 1_000_000) * rates["input"]
    output_cost = (output_tokens / 1_000_000) * rates["output"]
  
    return {
        "input_cost": input_cost,
        "output_cost": output_cost,
        "total_cost": input_cost + output_cost
    }

# Integrar c√°lculo de custos
def create_cost_aware_chain():
    def process_with_costs(inputs):
        token_result = token_chain.invoke(inputs)
        costs = calculate_costs(
            token_result['input_tokens'],
            token_result['output_tokens'],
            model_name
        )
    
        return {
            **token_result,
            **costs
        }
  
    return RunnableLambda(process_with_costs)
```

**üí° Sum√°rio:** Implementamos um sistema de c√°lculo de custos baseado em diferentes modelos OpenAI, convertendo o n√∫mero de tokens em valores monet√°rios para melhor controle or√ßament√°rio.

## Streaming e Processamento Ass√≠ncrono

### 1. Streaming de Resultados

```python
# Streaming s√≠ncrono
for chunk in chain.stream({
    "cbo_code": "2142-05", 
    "description": "Engenheiro de software"
}):
    print(chunk, end="", flush=True)
```

**üí° Sum√°rio:** O streaming permite ver resultados em tempo real conforme s√£o gerados, √∫til para respostas longas ou quando voc√™ quer feedback imediato do progresso.

### 2. Processamento Ass√≠ncrono

```python
import asyncio

async def async_processing():
    # Processamento ass√≠ncrono individual
    result = await chain.ainvoke({
        "cbo_code": "2142-05", 
        "description": "Engenheiro de software"
    })
    print(result)
  
    # Processamento em lote ass√≠ncrono
    results = await chain.abatch(inputs)
    return results

# Executar processamento ass√≠ncrono
async def main():
    results = await async_processing()
    print(f"Processados {len(results)} CBOs")

# asyncio.run(main())
```

**üí° Sum√°rio:** O processamento ass√≠ncrono com `ainvoke` e `abatch` permite m√°xima efici√™ncia ao processar m√∫ltiplos requests simult√¢neos, especialmente importante para grandes volumes de dados.

## Configura√ß√£o Avan√ßada com RunnableConfig

### 1. Callbacks e Monitoramento

```python
from langchain_core.callbacks import BaseCallbackHandler

class TokenUsageCallback(BaseCallbackHandler):
    def __init__(self):
        self.total_tokens = 0
        self.total_cost = 0.0
  
    def on_llm_end(self, response, **kwargs):
        if hasattr(response, 'llm_output') and response.llm_output:
            usage = response.llm_output.get('token_usage', {})
            tokens = usage.get('total_tokens', 0)
            self.total_tokens += tokens
            print(f"Tokens usados: {tokens}")

# Usar callback
callback = TokenUsageCallback()

result = chain.invoke(
    {"cbo_code": "2142-05", "description": "Engenheiro de software"},
    config={"callbacks": [callback]}
)

print(f"Total de tokens: {callback.total_tokens}")
```

**üí° Sum√°rio:** Callbacks permitem interceptar e monitorar eventos durante a execu√ß√£o das chains, essencial para logging, debugging e coleta de m√©tricas em ambiente de produ√ß√£o.

### 2. Tags e Metadados

```python
# Configurar chain com tags e metadados
result = chain.invoke(
    {"cbo_code": "2142-05", "description": "Engenheiro de software"},
    config={
        "tags": ["cbo-analysis", "production"],
        "metadata": {"batch_id": "batch_001", "priority": "high"}
    }
)
```

**üí° Sum√°rio:** Tags e metadados permitem categorizar e rastrear execu√ß√µes espec√≠ficas, facilitando an√°lise posterior e organiza√ß√£o de logs em sistemas de monitoramento.

## Exemplo Completo: Processamento em Escala

```python
import pandas as pd
from tqdm import tqdm

class CBOProcessor:
    def __init__(self, model_name="gpt-4o", batch_size=10):
        self.model_name = model_name
        self.batch_size = batch_size
        self.setup_chain()
    
    def setup_chain(self):
        """Configura a chain de processamento"""
        api_key = os.getenv("OPENAI_API_KEY")
        model = ChatOpenAI(model=self.model_name, temperature=0, api_key=api_key)
    
        prompt_template = create_cbo_prompt_template()
        output_parser = StrOutputParser()
    
        self.chain = prompt_template | model | output_parser
        self.map_chain = self.chain.map()
  
    def process_dataframe(self, df):
        """Processa DataFrame com dados de CBO"""
        # Preparar inputs
        inputs = []
        for _, row in df.iterrows():
            inputs.append({
                "cbo_code": row['codigo'],
                "description": row['termo']
            })
    
        # Processar em lotes
        results = []
        total_batches = len(inputs) // self.batch_size + (1 if len(inputs) % self.batch_size else 0)
    
        for i in tqdm(range(0, len(inputs), self.batch_size), desc="Processando lotes"):
            batch = inputs[i:i + self.batch_size]
            batch_results = self.map_chain.invoke(batch)
            results.extend(batch_results)
    
        return results
  
    def save_results(self, results, df, filename="resultados.csv"):
        """Salva resultados em arquivo"""
        results_df = pd.DataFrame(results)
        results_df['cbo'] = df['codigo'].tolist()[:len(results)]
        results_df['descricao'] = df['termo'].tolist()[:len(results)]
    
        results_df.to_csv(filename, index=False)
        print(f"Resultados salvos em {filename}")

# Uso da classe
if __name__ == "__main__":
    # Carregar dados
    df = pd.read_csv('lista-cbo.csv')
  
    # Inicializar processador
    processor = CBOProcessor(model_name="gpt-4o", batch_size=5)
  
    # Processar dados
    results = processor.process_dataframe(df.head(20))  # Teste com 20 registros
  
    # Salvar resultados
    processor.save_results(results, df.head(20))
```

**üí° Sum√°rio:** Esta classe `CBOProcessor` encapsula todos os conceitos anteriores em uma solu√ß√£o pronta para produ√ß√£o, incluindo processamento em lotes, monitoramento de progresso e controle de recursos.

## Melhores Pr√°ticas

### 1. Gest√£o de Rate Limits

```python
import time
from functools import wraps

def rate_limit(calls_per_minute=60):
    def decorator(func):
        last_called = [0.0]
    
        @wraps(func)
        def wrapper(*args, **kwargs):
            elapsed = time.time() - last_called[0]
            left_to_wait = 60.0 / calls_per_minute - elapsed
            if left_to_wait > 0:
                time.sleep(left_to_wait)
            ret = func(*args, **kwargs)
            last_called[0] = time.time()
            return ret
        return wrapper
    return decorator

# Aplicar rate limiting
@rate_limit(calls_per_minute=60)
def limited_invoke(chain, input_data):
    return chain.invoke(input_data)
```

**üí° Sum√°rio:** Rate limiting √© essencial para evitar exceder os limites da API OpenAI. Este decorator implementa um controle autom√°tico de frequ√™ncia de chamadas.

**üí° Sum√°rio:** Rate limiting √© essencial para evitar exceder os limites da API OpenAI. Este decorator implementa um controle autom√°tico de frequ√™ncia de chamadas.

### 2. Tratamento de Erros

```python
def create_resilient_chain():
    def safe_process(inputs):
        try:
            return chain.invoke(inputs)
        except Exception as e:
            print(f"Erro ao processar {inputs.get('cbo_code', 'unknown')}: {e}")
            return {
                "error": str(e),
                "cbo_code": inputs.get('cbo_code'),
                "success": False
            }
  
    return RunnableLambda(safe_process)

resilient_chain = create_resilient_chain()
```

**üí° Sum√°rio:** Tratamento robusto de erros evita que falhas pontuais interrompam o processamento completo, retornando informa√ß√µes estruturadas sobre problemas encontrados.

### 3. Cache de Resultados

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_invoke(cbo_code, description):
    return chain.invoke({
        "cbo_code": cbo_code,
        "description": description
    })
```

**üí° Sum√°rio:** Cache LRU evita reprocessamento desnecess√°rio de inputs id√™nticos, economizando tokens e tempo, especialmente √∫til quando h√° dados duplicados no dataset.

## Monitoramento e Debugging

### 1. Inspe√ß√£o da Chain

```python
# Visualizar estrutura da chain
print(chain.get_graph().print_ascii())

# Obter prompts utilizados
prompts = chain.get_prompts()
for prompt in prompts:
    print(prompt)
```

**üí° Sum√°rio:** Ferramentas de inspe√ß√£o ajudam a entender a estrutura interna das chains e verificar se os prompts est√£o sendo constru√≠dos corretamente antes da execu√ß√£o.

### 2. Logging Detalhado

```python
import logging

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Chain com logging
def logged_chain_invoke(inputs):
    logger.info(f"Processando CBO: {inputs.get('cbo_code')}")
    start_time = time.time()
  
    result = chain.invoke(inputs)
  
    duration = time.time() - start_time
    logger.info(f"Conclu√≠do em {duration:.2f}s")
  
    return result

logged_chain = RunnableLambda(logged_chain_invoke)
```

**üí° Sum√°rio:** Logging estruturado fornece visibilidade completa sobre o processo de execu√ß√£o, incluindo tempos de resposta e identifica√ß√£o de gargalos de performance.

## Considera√ß√µes de Performance

1. **Paraleliza√ß√£o**: Use `map()` para processamento paralelo
2. **Batch Size**: Encontre o equil√≠brio entre throughput e rate limits
3. **Caching**: Implemente cache para evitar reprocessamento
4. **Monitoring**: Monitore tokens e custos constantemente
5. **Error Handling**: Implemente retry logic e tratamento de falhas

## üçé Exemplo Pr√°tico

Para ver todos esses conceitos em a√ß√£o, execute o arquivo `aprenda.py` que demonstra o processamento de 50 frutas:

```bash
python aprenda.py
```

Este exemplo mostra:

- Compara√ß√£o entre processamento sequencial vs paralelo
- Tracking de tokens e custos em tempo real
- Processamento em lotes com barra de progresso
- Salvamento de resultados estruturados
- Estat√≠sticas detalhadas de performance

üìñ **Veja o arquivo `README-exemplo-frutas.md` para instru√ß√µes detalhadas**

## Conclus√£o

Os LangChain Runnables oferecem uma interface poderosa e flex√≠vel para construir pipelines de processamento com OpenAI. A combina√ß√£o de composabilidade, paraleliza√ß√£o e configurabilidade torna poss√≠vel processar prompts em escala de forma eficiente e controlada.

Para projetos de classifica√ß√£o como o de CBOs, essa abordagem permite:

- Processamento eficiente de grandes volumes de dados
- Controle preciso de custos e tokens
- Monitoramento detalhado do pipeline
- Facilidade de manuten√ß√£o e extens√£o do c√≥digo
